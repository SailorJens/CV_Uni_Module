{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2827684",
   "metadata": {},
   "source": [
    "# Face Detection Pipeline\n",
    "\n",
    "I am building a Face Detection Pipeline using the Open CV frontal face Haar cascade detection model (as we did in week 4). \n",
    "\n",
    "I will improve the output in two steps:\n",
    "1. Remove multiple boxes for the same face using Non-Maximum Suppression (NMS).\n",
    "2. Verify that each box actually contains a face with our trained HIOG feature classifier from week 5. \n",
    "\n",
    "\n",
    "I will measure the quality of the approaches with the WIDER FACE dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b99341",
   "metadata": {},
   "source": [
    "## Ingest the WIDER FACE label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0--Parade/0_Parade_marchingband_1_849.jpg', [(449, 330, 122, 149)])\n",
      "('0--Parade/0_Parade_Parade_0_904.jpg', [(361, 98, 263, 339)])\n",
      "('0--Parade/0_Parade_marchingband_1_799.jpg', [(78, 221, 7, 8), (78, 238, 14, 17), (113, 212, 11, 15), (134, 260, 15, 15), (163, 250, 14, 17), (201, 218, 10, 12), (182, 266, 15, 17), (245, 279, 18, 15), (304, 265, 16, 17), (328, 295, 16, 20), (389, 281, 17, 19), (406, 293, 21, 21), (436, 290, 22, 17), (522, 328, 21, 18), (643, 320, 23, 22), (653, 224, 17, 25), (793, 337, 23, 30), (535, 311, 16, 17), (29, 220, 11, 15), (3, 232, 11, 15), (20, 215, 12, 16)])\n",
      "('0--Parade/0_Parade_marchingband_1_117.jpg', [(69, 359, 50, 36), (227, 382, 56, 43), (296, 305, 44, 26), (353, 280, 40, 36), (885, 377, 63, 41), (819, 391, 34, 43), (727, 342, 37, 31), (598, 246, 33, 29), (740, 308, 45, 33)])\n",
      "('0--Parade/0_Parade_marchingband_1_778.jpg', [(27, 226, 33, 36), (63, 95, 16, 19), (64, 63, 17, 18), (88, 13, 16, 15), (231, 1, 13, 13), (263, 122, 14, 20), (367, 68, 15, 23), (198, 98, 15, 18), (293, 161, 52, 59), (412, 36, 14, 20), (441, 23, 17, 13), (475, 40, 14, 21), (510, 23, 14, 17), (576, 30, 16, 15), (577, 71, 16, 21), (595, 94, 16, 20), (570, 126, 13, 16), (645, 171, 52, 58), (719, 98, 11, 15), (791, 154, 54, 49), (884, 97, 16, 21), (898, 48, 15, 21), (945, 89, 15, 20), (922, 38, 15, 16), (743, 71, 11, 18), (841, 18, 16, 16), (980, 56, 13, 20), (1001, 107, 14, 13), (488, 2, 12, 18), (586, 1, 15, 17), (669, 1, 12, 15), (744, 2, 18, 15), (803, 3, 18, 17), (294, 2, 11, 10), (203, 0, 13, 14)])\n"
     ]
    }
   ],
   "source": [
    "# The label data text file is structured as follows:\n",
    "# - All image data are listed sequentially, with no blank lines in between.\n",
    "# - The first line contains the relative path to the image.\n",
    "# - The second line contains the number of faces in the image.\n",
    "# - The subsequent lines contain the bounding box information for each face, formatted as \"x y w h\", where (x, y) is the top-left corner of the bounding box, and w and h are the width and height of the bounding box, respectively.\n",
    "# - There are additional values on each box line, but we will ignore them for this task.\n",
    "# - An image with 0 faces is followed by one row of 0 0 0 0 \n",
    "\n",
    "images = []\n",
    "\n",
    "path_to_file = \"data/wider_face_split/wider_face_train_bbx_gt.txt\"\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    line = f.readline()\n",
    "    # Stop when no more lines to read\n",
    "    while line:\n",
    "        # First line in an image block is the image path\n",
    "        image_path = line.strip()\n",
    "        # Second line is the number of faces in the image\n",
    "        num_faces = int(f.readline().strip())\n",
    "        faces = []\n",
    "        # store the bounding box information for each face in the image\n",
    "        # note that if the number of faces is 0, this block is not executed\n",
    "        for _ in range(num_faces):\n",
    "            face_info = f.readline().strip().split()\n",
    "            x, y, w, h = map(int, face_info[:4])\n",
    "            faces.append((x, y, w, h))\n",
    "        # Handling the case of 0 faces: Ignore the next line which contains \"0 0 0 0\"\n",
    "        if num_faces == 0:\n",
    "            line = f.readline()\n",
    "        # Add image data to the list\n",
    "        images.append((image_path, faces))\n",
    "        # Read the next image path for the next iteration (or to end the loop)\n",
    "        line = f.readline()\n",
    "\n",
    "# the format of each entry is (image_path, [(x1, y1, w1, h1), (x2, y2, w2, h2), ...])\n",
    "# hence, number of faces in the image can be obtained by len(faces) for each entry, e.g., len(images[0][1]) gives the number of faces in the first image        \n",
    "# print the first 5 images and their faces for validation\n",
    "for image in images[:5]:\n",
    "    print(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a267bbf",
   "metadata": {},
   "source": [
    "## IoU calculator\n",
    "\n",
    "In order to identify whether two boxes refer to the same object (face), I calculate the \"Intersection over Union\" measure which calculates the overlap to total (union) size ratio. \n",
    "\n",
    "The measure calculates the intersection of two boxes, as well as the area that is covered by both boxes together (i.e. area of both boxes minus the intersection area to not count that twice.)\n",
    "\n",
    "If two boxes are identical IoU is 1 (intersection = union); if they do not overlap, IoU is 0 (intersection is 0).\n",
    "\n",
    "As different models / labelling may apply different padding to the faces, and boxes may be moved by some pixels, exact box matching cannot be expected. A standard measure to decide that two boxes box the same object is IoU > 0.5 which I will apply here. However, that value is to a degree arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9656be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IoU(boxA, boxB):\n",
    "    # boxA and boxB are in the format (x, y, w, h)\n",
    "\n",
    "    # first calculate the (x, y) coordinates of the intersection rectangle\n",
    "    i_x_top_left = max(boxA[0], boxB[0])\n",
    "    i_y_top_left = max(boxA[1], boxB[1])\n",
    "    i_x_bottom_right = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    i_y_bottom_right = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "\n",
    "    # Compute the area of intersection rectangle\n",
    "    # If the rectangles do not overlap, i_x_bottom_right - i_x_top_left \n",
    "    # or i_y_bottom_right - i_y_top_left will be negative, \n",
    "    # so we take max with 0 to ensure non-negative area\n",
    "    i_width = max(0, i_x_bottom_right - i_x_top_left)\n",
    "    i_height = max(0, i_y_bottom_right - i_y_top_left)\n",
    "    # If no overlap, the intersection area will be 0\n",
    "    i_area = i_width * i_height\n",
    "\n",
    "    # Compute the area of both boxes based on width*height\n",
    "    boxA_area = boxA[2] * boxA[3]\n",
    "    boxB_area = boxB[2] * boxB[3]\n",
    "\n",
    "    # Compute the intersection over union by taking the intersection area \n",
    "    # and dividing it by the sum of prediction + ground-truth areas - the interesection area\n",
    "    iou = i_area / float(boxA_area + boxB_area - i_area)\n",
    "\n",
    "    return iou  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22e715df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21951219512195122\n"
     ]
    }
   ],
   "source": [
    "print(calculate_IoU((10, 10, 50, 50), (30, 30, 50, 50)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
